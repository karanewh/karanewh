# Kara Newhouse Data Journalism Portfolio
## Overview
This repository includes ...

## Essential_and_exposed: Analysis of workplace COVID-19 outbreaks

<p>During the first year of the coronavirus pandemic in the U.S., oversight of worker safety during the COVID-19 pandemic was splintered among federal OSHA, state agencies and even local boards of health. A team at the Howard Center for Investigative Journalism found that essential workers at places like Walmart paid the price. The <a href="https://apnews.com/article/coronavirus-pandemic-health-business-caf5e31d883a18deae6cd367a5ee8978">Associated Press</a> ran our story, and it was picked up by newspapers across the country, including The Washington Post.</p>
<p>From records acquisition, to analysis in Rstudio, to many late-night rounds of writing, revising and fact checking, I was deeply involved in every aspect of the data work on this project. To obtain data on workplace COVID-19 outbreaks from state health departments, I coordinated a 50-person public records campaign. As we acquired the records, I co-led a team of seven reporters in cleaning, wrangling and analyzing the workplace COVID-19 data. We examined the data with an eye toward patterns and outliers within states and a focus on outbreaks at Walmart store and distribution centers. We also acquired and analyzed at complaints filed with the federal Occupational Safety and Health Administration.</p>
<p>The data analysis for this project was particularly complex, as states differed in their definitions of "workplace outbreak" across multiple variables, and some did not even use the term "outbreak." My work thus required not only coding and statistical skills but strong news judgment and ethical decisionmaking. To see the 13 data findings that shaped the final story, along with their underlying code, please visit our <a href="https://howard-center-investigations.github.io/essential_and_exposed/osha_walmart/index.html">line-by-line fact check</a>.</p>


## Newspaper lineage web scraper
<p>A colleague and I built a web scraper that uses the Library of Congress Chronicling America database to trace the lineages of all current U.S. newspapers. We wrote the code for the scraper in Python using Selenium and exported the resulting files to R for further manipulation and analysis. This scraper laid the foundation for analysis of historical newspaper coverage in a forthcoming investigative series and news app from the Howard Center for investigative Journalism.</p>


## Data-driven education reporting
<p>Prior to my graduate program, I used a data-driven approach to connect public policy to peopleâ€™s lives as an education beat reporter in Lancaster, Pennsylvania.</p>
<p>For example, as frustration over standardized testing surged nationwide, I obtained data from the state Department of Education and <a href="https://lancasteronline.com/news/local/skipping-the-tests-pennsylvania-opt-out-numbers-doubled-last-year/article_f67ad248-b2e9-11e4-80ec-3fec00371a7d.html">reported on a statewide spike in families opting children out of mandated assessments</a>, with my county as one of the hotspots. This finding complemented prior reporting I had done on local parents organizing around opt-outs. After my reporting on the statewide opt-out spike, other education reporters across Pennsylvania sought the data I had obtained and localized the topic to their regions.</p>
<p>I conducted my analyses for that story and others using self-taught methods in Google Sheets. I also experimented with building interactive maps via Fusion Tables and regularly created my own graphics for stories using free web tools.</p>
